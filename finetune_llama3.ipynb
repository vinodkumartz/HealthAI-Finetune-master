{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:40:59.408527Z","iopub.execute_input":"2024-08-28T18:40:59.408869Z","iopub.status.idle":"2024-08-28T18:42:09.087861Z","shell.execute_reply.started":"2024-08-28T18:40:59.408821Z","shell.execute_reply":"2024-08-28T18:42:09.086610Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nimport json\nimport torch\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom unsloth import FastLanguageModel","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:42:18.234795Z","iopub.execute_input":"2024-08-28T18:42:18.235201Z","iopub.status.idle":"2024-08-28T18:42:28.237482Z","shell.execute_reply.started":"2024-08-28T18:42:18.235162Z","shell.execute_reply":"2024-08-28T18:42:28.236235Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:42:28.239519Z","iopub.execute_input":"2024-08-28T18:42:28.240148Z","iopub.status.idle":"2024-08-28T18:42:28.265570Z","shell.execute_reply.started":"2024-08-28T18:42:28.240111Z","shell.execute_reply":"2024-08-28T18:42:28.264489Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6ba3f2b39ad4fbdae2a564beee1dc14"}},"metadata":{}}]},{"cell_type":"code","source":"# Defining the configuration for the base model, LoRA and training\nconfig = {\n    \"hugging_face_username\":\"pradeep9322\",\n    \"model_config\": {\n        \"base_model\":\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # The base model\n        \"finetuned_model\":\"pradeep9322/llama3-Medical-Chat\", # The fine-tuned model\n        \"max_seq_length\": 2048, # The maximum sequence length\n        \"dtype\":torch.float16, # The data type\n        \"load_in_4bit\": True, # Load the model in 4-bit\n    },\n    \"lora_config\": {\n      \"r\": 16, # The number of LoRA layers 8, 16, 32, 64\n      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n      \"lora_alpha\":16, # The alpha value for LoRA\n      \"lora_dropout\":0, # The dropout value for LoRA\n      \"bias\":\"none\", # The bias for LoRA\n      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n      \"use_rslora\":False, # Use RSLora\n      \"use_dora\":False, # Use DoRa\n      \"loftq_config\":None # The LoFTQ configuration\n    },\n    \"training_dataset\":{\n        \"name\":\"pradeep9322/LLaMA3-Instruct-Medical\", # The dataset name(huggingface/datasets)\n        \"split\":\"train\", # The dataset split\n        \"input_field\":\"prompt\", # The input field\n    },\n    \"training_config\": {\n        \"per_device_train_batch_size\": 2, # The batch size # Mini batch gradiant descent\n        \"gradient_accumulation_steps\": 4, # The gradient accumulation steps\n        \"warmup_steps\": 5, # The warmup steps\n        \"max_steps\":500, # The maximum steps (0 if the epochs are defined)\n        \"num_train_epochs\": 0, # The number of training epochs(0 if the maximum steps are defined)\n        \"learning_rate\": 2e-4, # The learning rate\n        \"fp16\": not torch.cuda.is_bf16_supported(), # The fp16\n        \"bf16\": torch.cuda.is_bf16_supported(), # The bf16\n        \"logging_steps\": 1, # The logging steps\n        \"optim\" :\"adamw_8bit\", # The optimizer\n        \"weight_decay\" : 0.01,  # The weight decay\n        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n        \"seed\" : 42, # The seed\n        \"output_dir\" : \"outputs\", # The output directory\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:43:12.519625Z","iopub.execute_input":"2024-08-28T18:43:12.520631Z","iopub.status.idle":"2024-08-28T18:43:12.531396Z","shell.execute_reply.started":"2024-08-28T18:43:12.520586Z","shell.execute_reply":"2024-08-28T18:43:12.530446Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Loading the model and the tokinizer for the model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = config.get(\"model_config\").get(\"base_model\"),\n    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n    dtype = config.get(\"model_config\").get(\"dtype\"),\n    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n)\n\n# Setup for QLoRA/LoRA peft of the base model\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = config.get(\"lora_config\").get(\"r\"),\n    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n    bias = config.get(\"lora_config\").get(\"bias\"),\n    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n    random_state = 42,\n    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:43:19.793875Z","iopub.execute_input":"2024-08-28T18:43:19.794696Z","iopub.status.idle":"2024-08-28T18:43:31.743239Z","shell.execute_reply.started":"2024-08-28T18:43:19.794651Z","shell.execute_reply":"2024-08-28T18:43:31.742417Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n     ","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:43:31.744700Z","iopub.execute_input":"2024-08-28T18:43:31.745029Z","iopub.status.idle":"2024-08-28T18:43:38.866974Z","shell.execute_reply.started":"2024-08-28T18:43:31.744993Z","shell.execute_reply":"2024-08-28T18:43:38.866165Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/393 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ef8e7e1d7b42cfb5d1db6b291aba59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/145M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6079ae68838a486ca62dbdd970c86909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/112165 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a3ae233e40d443fbc8804b7550f0c7e"}},"metadata":{}}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_train,\n    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n        fp16 = config.get(\"training_config\").get(\"fp16\"),\n        bf16 = config.get(\"training_config\").get(\"bf16\"),\n        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n        optim = config.get(\"training_config\").get(\"optim\"),\n        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n        seed = 42,\n        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:43:43.931815Z","iopub.execute_input":"2024-08-28T18:43:43.932510Z","iopub.status.idle":"2024-08-28T18:44:33.239458Z","shell.execute_reply.started":"2024-08-28T18:43:43.932452Z","shell.execute_reply":"2024-08-28T18:44:33.238504Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/112165 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3183323eaa4a7287cab627f595dbce"}},"metadata":{}}]},{"cell_type":"code","source":"# Memory statistics before training\ngpu_statistics = torch.cuda.get_device_properties(0)\nreserved_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\nmax_memory = round(gpu_statistics.total_memory / 1024**3, 2)\nprint(f\"Reserved Memory: {reserved_memory}GB\")\nprint(f\"Max Memory: {max_memory}GB\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:44:38.526752Z","iopub.execute_input":"2024-08-28T18:44:38.527494Z","iopub.status.idle":"2024-08-28T18:44:38.534389Z","shell.execute_reply.started":"2024-08-28T18:44:38.527453Z","shell.execute_reply":"2024-08-28T18:44:38.533421Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Reserved Memory: 5.61GB\nMax Memory: 14.74GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training the model\ntrainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-28T18:44:50.126305Z","iopub.execute_input":"2024-08-28T18:44:50.126705Z","iopub.status.idle":"2024-08-28T20:02:27.221286Z","shell.execute_reply.started":"2024-08-28T18:44:50.126667Z","shell.execute_reply":"2024-08-28T20:02:27.220332Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 112,165 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 500\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:17:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.742400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>4.168000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.386400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.600600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.442500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.173800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.926200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.717200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.654200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.577000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.726400</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.437900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.461600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.432400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.264500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.420700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.464700</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.517100</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.736500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.347400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.577000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.603800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.347400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.431200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.353300</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.396000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.553000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.434400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.135700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.203500</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.325500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.359400</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>2.237300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.208800</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.458500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.280500</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.330700</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.309000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>2.450300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.377300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.333600</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>2.637700</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.100200</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.283000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.600200</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.338600</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.309000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.425200</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.416900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.248300</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>2.283300</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>2.245100</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>2.188600</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>2.231800</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.279800</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>2.292100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>2.362600</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.341300</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.383700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.331300</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>2.098900</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>2.276500</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>2.348900</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.919200</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.405300</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>2.315700</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>2.542600</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>2.112600</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>2.012600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.341800</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>2.360100</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>2.283700</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>2.277500</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>2.258300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.136400</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>2.237400</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>2.476100</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>2.425400</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>2.145400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.374200</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>2.403400</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>2.211500</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>2.193600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>2.233500</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>2.328500</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>1.978700</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>2.190300</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>2.381100</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>2.101500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.308300</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>2.525100</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>2.339600</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>2.032800</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>2.270800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>2.051700</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>2.457500</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>2.311900</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>2.245400</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>2.213100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.120200</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.965100</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>2.342700</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>2.245800</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>2.324500</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>2.223500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>2.242000</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>2.228700</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>2.143900</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>2.237000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.096400</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>2.251700</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>2.176700</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>2.384400</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>2.354400</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>2.139200</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>2.204500</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>2.386100</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>2.271100</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>2.320000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.328400</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>2.206600</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>2.163700</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>2.223400</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>2.264600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.259300</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>2.170200</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>2.226300</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>2.078800</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>2.346300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.302300</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>2.205800</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>2.413500</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>2.266400</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>2.395900</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>2.154400</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>1.977200</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>2.033100</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>2.411000</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>2.128200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.097700</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>2.212800</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>2.134600</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>2.304200</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>2.445100</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>2.095400</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>2.341800</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>2.118400</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>2.192700</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>2.076600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.323900</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>2.279900</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>2.248600</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>2.120100</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>2.129000</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>2.145600</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>2.278300</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>2.451500</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>2.294300</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>2.267300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.240000</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>2.356300</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>2.070700</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>2.444600</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>2.338000</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>2.295900</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>2.007300</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>2.095400</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>2.150700</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>1.944000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.290700</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>2.381000</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>2.247300</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>2.154200</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>2.616000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.555500</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>1.958300</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>2.271500</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>2.225200</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>2.191900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.093600</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>2.285800</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>2.306200</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>2.200900</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>2.195100</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>2.166600</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>2.203900</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>2.344800</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>2.152400</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>2.072000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.214300</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>2.159000</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>1.916100</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>2.029600</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>2.150400</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>2.358900</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>2.149200</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>2.195000</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>2.069000</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>2.245300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.417700</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>2.231000</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>2.294400</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>2.165700</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>2.015500</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>2.183600</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>2.134100</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>2.007200</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>2.019900</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>2.534500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.375900</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>2.011900</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>2.065500</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>2.122300</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>2.147100</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>2.128400</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>2.183400</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>2.163900</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>2.249200</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>2.172100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.231500</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>2.230600</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>2.371300</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>2.154600</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>2.230300</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.116900</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>2.143200</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>2.315200</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>2.218200</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>2.099300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.182400</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>2.219300</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>2.044000</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>2.192100</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>2.306000</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>1.972400</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>2.153600</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>2.008400</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>2.444700</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>2.410500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.960000</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>2.390100</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>2.254900</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>2.219400</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>2.219100</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>2.156100</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>1.973500</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>2.268800</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>2.281200</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>2.439100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.977100</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>2.150900</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>2.284200</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>1.961800</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>2.466700</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>2.426700</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>2.027300</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>2.167200</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>2.296500</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>2.111600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.254000</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>2.378700</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>2.360000</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>2.134900</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>1.967300</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>2.281300</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>2.384000</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>2.326500</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>2.133500</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>1.933700</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.326300</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>2.011700</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>2.412100</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>1.949600</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>2.412700</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.292200</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>2.094700</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>2.318200</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>1.955700</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>2.129400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.156100</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>2.103700</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>2.211400</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>2.313100</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>2.122900</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>2.085100</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>2.121900</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>2.231900</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>2.266300</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>2.229800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.068800</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>2.207900</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>2.163100</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>1.877600</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>2.074500</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>2.075700</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>2.093100</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>2.336600</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>2.355700</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>2.328800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.156800</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>2.078000</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>2.119700</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>2.008100</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>2.005900</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>2.057100</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>2.370100</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>2.198300</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>2.170400</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>2.206900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.149300</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>2.027600</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>2.337300</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>2.305300</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>2.012800</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>1.991800</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>2.173500</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>1.740300</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>2.352500</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>2.319900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.190100</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>2.096300</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>2.205800</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>2.007800</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>2.074900</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.936000</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>2.090300</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>1.964400</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>2.035200</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>2.416600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.302500</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>2.128800</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>2.066100</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>2.153200</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>2.041800</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>2.300700</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>2.038100</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>2.085400</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>2.137700</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>2.100500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.120200</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>1.942000</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>2.174100</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>2.242500</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>2.204100</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>2.230400</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>2.177700</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>2.144100</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>2.140500</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>2.150100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.151000</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>2.063800</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>1.985200</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>2.187200</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>2.288000</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>2.066100</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>2.099800</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>2.216800</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>2.126700</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>1.807700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.335000</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>2.312400</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>2.253500</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>2.148500</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>2.141000</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>2.158300</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>2.170300</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>2.058200</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>2.091300</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>2.275900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.957800</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>2.042900</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>2.099200</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>2.054200</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>2.093400</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.157800</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>2.102700</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>2.120600</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>2.131800</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>2.333300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.209800</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>2.205800</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>2.181200</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>2.040300</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>2.422400</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>2.003600</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>2.102600</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>2.284600</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>2.445300</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>2.250800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.004800</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>2.028100</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>2.046600</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>2.101100</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>2.286000</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>2.449800</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>2.229600</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>2.102600</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>2.154600</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>1.721300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.280500</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>2.041100</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>2.501000</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>1.994500</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>2.099200</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>2.371400</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>2.373100</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>2.314600</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>2.284600</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>2.105200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.033100</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>2.250500</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>2.462600</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>2.258900</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>2.156800</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>2.424500</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>2.027600</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>2.305500</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>2.074300</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>2.178500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.042200</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>2.257000</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>2.178800</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>2.226700</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>2.090900</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.324300</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>2.058900</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>2.052200</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>2.002800</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>1.932700</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.201300</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>1.986900</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>2.225100</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>2.177000</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>2.244700</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>2.182400</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>1.942300</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>1.881800</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>2.295700</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>2.108200</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.980500</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>2.344900</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>2.320100</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>2.237200</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>2.416300</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>2.186800</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>2.058300</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>2.138400</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>2.221600</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>2.357100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.319900</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>2.288300</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>2.092300</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>2.234700</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>2.242200</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>2.101000</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>2.062000</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>2.315300</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>2.059000</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>2.050500</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.079000</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>2.038800</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>2.221200</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>1.828300</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>2.000400</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>2.390200</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>2.349600</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>2.048800</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>1.987000</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>2.449400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.027900</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>2.114300</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>2.237300</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>1.794500</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>2.272700</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.058700</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>2.182800</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>2.246400</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>2.203700</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>2.370900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.186500</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>2.194800</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>2.209000</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>2.197000</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>2.152900</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>2.253700</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>2.106800</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>1.930700</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>1.946600</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>2.256700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>2.211600</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>2.218800</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>2.242300</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>2.226300</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>2.150300</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>2.205800</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>2.107500</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>2.198100</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>2.327200</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>1.942800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.159100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Memory statistics after training\nused_memory = round(torch.cuda.max_memory_allocated() / 1024**3, 2)\nused_memory_lora = round(used_memory - reserved_memory, 2)\nused_memory_persentage = round((used_memory / max_memory) * 100, 2)\nused_memory_lora_persentage = round((used_memory_lora / max_memory) * 100, 2)\nprint(f\"Used Memory: {used_memory}GB ({used_memory_persentage}%)\")\nprint(f\"Used Memory for training(fine-tuning) LoRA: {used_memory_lora}GB ({used_memory_lora_persentage}%)\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T20:04:13.910318Z","iopub.execute_input":"2024-08-28T20:04:13.910779Z","iopub.status.idle":"2024-08-28T20:04:13.918205Z","shell.execute_reply.started":"2024-08-28T20:04:13.910736Z","shell.execute_reply":"2024-08-28T20:04:13.917352Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Used Memory: 8.48GB (57.53%)\nUsed Memory for training(fine-tuning) LoRA: 2.87GB (19.47%)\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Saving the trainer stats\nwith open(\"trainer_stats.json\", \"w\") as f:\n    json.dump(trainer_stats, f, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T20:04:36.690799Z","iopub.execute_input":"2024-08-28T20:04:36.691666Z","iopub.status.idle":"2024-08-28T20:04:36.696868Z","shell.execute_reply.started":"2024-08-28T20:04:36.691618Z","shell.execute_reply":"2024-08-28T20:04:36.695805Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenizing the input and generating the output\ninputs = tokenizer(\n[\n    \"<|start_header_id|>system<|end_header_id|> Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<|eot_id|>\"\n\n], return_tensors = \"pt\").to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T20:05:11.645670Z","iopub.execute_input":"2024-08-28T20:05:11.646686Z","iopub.status.idle":"2024-08-28T20:05:11.652589Z","shell.execute_reply.started":"2024-08-28T20:05:11.646646Z","shell.execute_reply":"2024-08-28T20:05:11.651731Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\n# Prepare the model for fast inference\nFastLanguageModel.for_inference(model)\n\n# Tokenizing the input\ninputs = tokenizer(\n    [\n        \"<|start_header_id|>system<|end_header_id|> Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<|eot_id|>\"\n    ],\n    return_tensors=\"pt\"\n).to(\"cuda\")\n\n# Generating the output\noutputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T20:07:28.832046Z","iopub.execute_input":"2024-08-28T20:07:28.833180Z","iopub.status.idle":"2024-08-28T20:07:40.761442Z","shell.execute_reply.started":"2024-08-28T20:07:28.833127Z","shell.execute_reply":"2024-08-28T20:07:40.760448Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[\"system Answer the question truthfully, you are a medical professional.user This is the question: Can you provide an overview of the lung's squamous cell carcinoma?assistantHi, Thanks for writing in. Squamous cell carcinoma of the lung is a type of lung cancer that is usually found in the central part of the lung. It is usually found in the larger airways of the lung. This type of cancer is also known as epidermoid carcinoma of the lung. It is a type of non-small cell lung cancer. It usually grows slowly and can be treated with surgery, chemotherapy and radiation therapy. It can also spread to other parts of the body such as the brain, liver, bones and lymph nodes. The symptoms of lung cancer can be coughing, wheezing, shortness of breath, fatigue, loss of weight, loss of appetite, chest pain, and pain in the back and shoulders. If you have any of these symptoms, you should consult a doctor.\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Locally saving the model and pushing it to the Hugging Face Hub (only LoRA adapters)\nmodel.save_pretrained(config.get(\"model_config\").get(\"finetuned_model\"))\nmodel.push_to_hub(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer = tokenizer)","metadata":{},"execution_count":null,"outputs":[]}]}